{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting cloud cover using neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "np.random.seed(12)\n",
    "\n",
    "import warnings\n",
    "#Comment this to turn on warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from model_comparison import model_comparison\n",
    "from resample import resample\n",
    "import algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import netCDF4 as n\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils import train_test_split\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "\n",
    "from deepNN import NeuralNetRegressor\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from utils import mean_squared_error, A_R2, NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading test\n",
    "path = \"./files/\"\n",
    "filenames = [\"specific_humidity_Europa_sp.nc\", \"relative_humidity_Europa_sp.nc\", \"pressure_Europa_sp.nc\",  \n",
    "             \"temperature_Europa_sp.nc\", \"total_cloud_cover_Europa_sp.nc\"]\n",
    "\n",
    "\n",
    "cloud = n.Dataset(path + filenames[-1], \"r\")\n",
    "relative = n.Dataset(path + filenames[1], \"r\")\n",
    "specific = n.Dataset(path + filenames[0], \"r\")\n",
    "pressure = n.Dataset(path + filenames[2], \"r\")\n",
    "temperature = n.Dataset(path + filenames[3], \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cloud.variables)\n",
    "tcc = cloud.variables[\"tcc\"][:][:][:].data\n",
    "\n",
    "# Retriving ground values, these are available at six different pressure levels. \n",
    "rel = relative.variables[\"r\"][:][:][:][:].data\n",
    "#level = relative.variables[\"level\"][:][0].data\n",
    "spe = specific.variables[\"q\"][:][:][:][:].data\n",
    "\n",
    "surf_pre = pressure.variables[\"sp\"][:][:][:].data\n",
    "temp = temperature.variables[\"t2m\"][:][:][:].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_inv(x): # sigmoid?\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "def logit(x):\n",
    "    return np.log((x + 1e-12)/(1+1e-12 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one certain timestep \n",
    "\n",
    "n_days = 7\n",
    "\n",
    "TCC = []\n",
    "REL = []\n",
    "SPE = []\n",
    "PRE = []\n",
    "TEMP = []\n",
    "\n",
    "\n",
    "for t in range(int(n_days*4)):\n",
    "    TCC.append(tcc[t][:][:].flatten())\n",
    "    REL.append(rel[t][0][:][:].flatten())\n",
    "    SPE.append(spe[t][0][:][:].flatten())\n",
    "    PRE.append(surf_pre[t][:][:].flatten())\n",
    "    TEMP.append(temp[t][:][:].flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =(np.array(TCC).flatten())\n",
    "temp = y[y<1]\n",
    "y[y>1] = temp.max()\n",
    "\n",
    "X = np.array([np.array(REL).flatten(), np.array(SPE).flatten(), np.array(PRE).flatten(), np.array(TEMP).flatten()])\n",
    "y = logit(np.array(TCC).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.99866855977416e-13, 1.0000000000009999)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(TCC).min(), np.array(TCC).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-26.93794050959591, 36.04365338911916)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.min(), y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((131516,), (131516,), (131516,), (131516,), (131516,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X[0]),np.shape(X[1]),np.shape(X[2]),np.shape(X[3]), np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape((len(y),1))\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((131516, 4), (131516, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X), np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nfit = scaler.fit(X_train)\\n\\nX_train = scaler.transform(X_train)\\nX_test = scaler.transform(X_test)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, split_size = 0.2)\n",
    "import sklearn.model_selection as s\n",
    "X_train, X_test, y_train, y_test = s.train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "\"\"\"from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "fit = scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((105212, 4), (26304, 4), (105212, 1), (26304, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train), np.shape(X_test), np.shape(y_train),  np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.04365338911916, -26.93794050959591)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.max(), y_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_nodes = [10,30,50,100, 500]\n",
    "eta = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "lmd = [0.0001, 0.001, 0.01, 0.1, 1.0, 10]\n",
    "epochs = [10,50,100]\n",
    "batch_s = [1,10,50]\n",
    "\n",
    "\n",
    "#for e in epochs:\n",
    "#    for b in batch_s:\n",
    "#        for et in eta:\n",
    "#            for n in n_nodes:\n",
    "model = NeuralNetRegressor(n_hidden = [30, 20],  \n",
    "                           epochs=100, \n",
    "                           eta=0.001, \n",
    "                           shuffle=True, \n",
    "                           batch_size=1,\n",
    "                           seed=None, \n",
    "                           alpha=0.0001, # dette er for relu ikke det samme som penaltien i scikit learn  \n",
    "                           activation='sigmoid')\n",
    "\n",
    "p = model.fit(X_train, y_train, X_test, y_test)\n",
    "l = model.predict(X_test)\n",
    "#print(\" \")\n",
    "#print( \" for epochs :\" + str(e) + \" for bactsize : \" + str(b) + \" learningrat e : \" + str(et) + \"noden in middle layer n: \" + str(n) + \" traininperformance ois \" + str(p.eval_['train_preform']) + \"validation performance is \"+ str(p.eval_['valid_preform']))\n",
    "#print(\" \")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p.eval_['train_preform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p.eval_['valid_preform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = p.eval_['cost_train']\n",
    "te = p.eval_['cost_test']\n",
    "\n",
    "x = np.arange(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "temp2 = []\n",
    "for i in range(len(x)):\n",
    "    temp.append(tr[i][0][0])\n",
    "    temp2.append(te[i][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "plt.plot(x,temp, label = \"cost train\")\n",
    "#plt.plot(x,temp2, label = \"cost test\")\n",
    "plt.legend()\n",
    "plt.savefig(\"results/figures/cost_nn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit MLP Regressor using one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#n_nodes = [10,30,50,100, 500]\n",
    "eta = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "lmd = [0.0001, 0.001, 0.01, 0.1, 1.0, 10]\n",
    "#epochs = [10,50,100] Scikit stopper p√• rett antall epochs\n",
    "batch_s = [1,10,50]\n",
    "\n",
    "n, p = X_train.shape\n",
    "\n",
    "#for e in epochs:\n",
    "\n",
    "for b in batch_s:\n",
    "    for et in eta:\n",
    "        for l in lmd:\n",
    "            mlp = MLPRegressor(hidden_layer_sizes=(30,), \n",
    "                               activation = 'logistic', # this is the sigmoid activation function\n",
    "                               solver = \"adam\", \n",
    "                               alpha = l, # penalty\n",
    "                               batch_size = b, \n",
    "                               learning_rate_init = et)\n",
    "\n",
    "            mlp.fit(X_train, y_train)\n",
    "            y_pred = mlp.predict(X_test)\n",
    "            #logistic activation uses the sigmoid function \n",
    "            mse = NRMSE(y_pred, y_test)\n",
    "            ajusted_r2 = A_R2(y_pred, y_test, n, p)\n",
    "            print(\" eta : \" + str(et) + \" lmd \"+ str(l) +\"    batch size : \" +  str(b)   + \"   mse is \" + str(mse) + \" a r2  \" + str(ajusted_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding regularization may result in a better preformance despite of the network architecture ..? Doen't apper to be so, but check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
