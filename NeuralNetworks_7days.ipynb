{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting cloud cover using neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "np.random.seed(12)\n",
    "\n",
    "import warnings\n",
    "#Comment this to turn on warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from model_comparison_2 import model_comparison\n",
    "from resample import resample\n",
    "import algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import netCDF4 as n\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils import train_test_split\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "\n",
    "from deepNN import NeuralNetRegressor\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from utils import mean_squared_error, A_R2, NRMSE, transforming_predictorspace, standardicing_responce\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading test\n",
    "path = \"./files/\"\n",
    "filenames = [\"specific_humidity_Europa_sp.nc\", \"relative_humidity_Europa_sp.nc\", \"pressure_Europa_sp.nc\",  \n",
    "             \"temperature_Europa_sp.nc\", \"total_cloud_cover_Europa_sp.nc\"]\n",
    "\n",
    "\n",
    "cloud = n.Dataset(path + filenames[-1], \"r\")\n",
    "relative = n.Dataset(path + filenames[1], \"r\")\n",
    "specific = n.Dataset(path + filenames[0], \"r\")\n",
    "pressure = n.Dataset(path + filenames[2], \"r\")\n",
    "temperature = n.Dataset(path + filenames[3], \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cloud.variables)\n",
    "tcc = cloud.variables[\"tcc\"][:][:][:].data\n",
    "\n",
    "# Retriving ground values, these are available at six different pressure levels. \n",
    "rel = relative.variables[\"r\"][:][:][:][:].data\n",
    "#level = relative.variables[\"level\"][:][0].data\n",
    "spe = specific.variables[\"q\"][:][:][:][:].data\n",
    "\n",
    "surf_pre = pressure.variables[\"sp\"][:][:][:].data\n",
    "temp = temperature.variables[\"t2m\"][:][:][:].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_inv(x): # sigmoid?\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "def logit(x):\n",
    "    return np.log((x + 1e-12)/(1+1e-12 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for one certain timestep \n",
    "\n",
    "n_days = 7\n",
    "\n",
    "TCC = []\n",
    "REL = []\n",
    "SPE = []\n",
    "PRE = []\n",
    "TEMP = []\n",
    "\n",
    "\n",
    "for t in range(int(n_days*4)):\n",
    "    TCC.append(tcc[t][:][:].flatten())\n",
    "    REL.append(rel[t][0][:][:].flatten())\n",
    "    SPE.append(spe[t][0][:][:].flatten())\n",
    "    PRE.append(surf_pre[t][:][:].flatten())\n",
    "    TEMP.append(temp[t][:][:].flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "y =(np.array(TCC).flatten())\n",
    "temp = y[y<1]\n",
    "y[y>1] = temp.max()\n",
    "print(y.min()>0)\n",
    "\n",
    "X = np.array([np.array(REL).flatten(), np.array(SPE).flatten(), np.array(PRE).flatten(), np.array(TEMP).flatten()])\n",
    "#y = logit(np.array(TCC).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize = (20,20))\n",
    "#plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.99866855977416e-13, 1.0000000000009999)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(TCC).min(), np.array(TCC).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.99866855977416e-13, 0.9999694810258271)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.min(), y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((131516,), (131516,), (131516,), (131516,), (131516,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X[0]),np.shape(X[1]),np.shape(X[2]),np.shape(X[3]), np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape((len(y),1))\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((131516, 4), (131516, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X), np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, split_size = 0.2)\n",
    "import sklearn.model_selection as s\n",
    "X_train, X_test, y_train, y_test = s.train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Kunne brukt fit transform\n",
    "fit = scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((105212, 4), (26304, 4), (105212, 1), (26304, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train), np.shape(X_test), np.shape(y_train),  np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999694810258271, 9.99866855977416e-13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.max(), y_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE HIDDEN LAYER - sigmoid - 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nr_epochs = 1000\n",
    "\n",
    "n_nodes = [10,30,50,100,200,300,400,500]\n",
    "\n",
    "# Array which contains all the \"calibrated models \".\n",
    "results = []\n",
    "\n",
    "for n in n_nodes:\n",
    "    model = NeuralNetRegressor(n_hidden = [n],  \n",
    "                               epochs=nr_epochs, \n",
    "                               eta=0.0001, # larger eta results in exploding gradients, becomes NaN.\n",
    "                               shuffle = True, \n",
    "                               batch_size = 10,\n",
    "                               seed = None, \n",
    "                               alpha = 0.0001, # dette er for relu ikke det samme som penaltien i scikit learn  \n",
    "                               activation = 'sigmoid')\n",
    "\n",
    "    p = model.fit(X_train, y_train, X_test, y_test)\n",
    "    model.predict(X_test)\n",
    "    results.append(p)\n",
    "    print(\"Finished model: \" + str(n) + \" mse \" + str(np.nanmean(p.eval_['valid_preform'])) + \" r2 \" + str(np.nanmean(p.eval_['valid_preform_r2'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "counter = 0\n",
    "for model in results:\n",
    "    mse_train = model.eval_['train_preform']\n",
    "    mse_test = model.eval_['valid_preform']\n",
    "    cost_train = model.eval_[\"cost_train\"]\n",
    "    r2_test = model.eval_['valid_preform_r2']\n",
    "\n",
    "    ax1 = plt.subplot(211)\n",
    "    plt.plot(r2_test, label = str(n_nodes[counter])) \n",
    "    plt.title(\"R2 score for different network architectures\", fontsize = 20)\n",
    "    plt.ylabel(\"R2 score\", fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Alle disse blir like.\n",
    "    ax2 = plt.subplot(212)\n",
    "    plt.plot(mse_test, label = str(n_nodes[counter])) \n",
    "    plt.title(\"MSE test for different one hidden layer architectures\", fontsize = 20)\n",
    "    plt.ylabel(\"MSE\", fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "plt.xlabel(\"Epochs\", fontsize = 15)\n",
    "plt.savefig(\"results/figures/NN_one_hidden_layer_sigmoid_1000epochs_eta0.0001_featured_scaled_7.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look the same for all models.\n",
    "error = results[0].model_error\n",
    "plt.hist(error)\n",
    "plt.title(\"Histogram error using sigmoid, 1000 epochs and one timestep of data\", fontsize = 15)\n",
    "plt.savefig(\"results/figures/NN_one_hidden_layer_sigmoid_1000epochs_eta0.0001_featured_scaled_1_error.png\")\n",
    "# They all look the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE HIDDEN LAYER - ELU - 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 1000\n",
    "\n",
    "n_nodes = [10, 30, 50, 100, 200, 300, 400, 500]\n",
    "\n",
    "# Array which contains all the \"calibrated models \".\n",
    "results = []\n",
    "\n",
    "for n in n_nodes:\n",
    "    model = NeuralNetRegressor(n_hidden = [n],  \n",
    "                               epochs=nr_epochs, \n",
    "                               eta=0.0001, # larger eta results in exploding gradients, becomes NaN.\n",
    "                               shuffle = True, \n",
    "                               batch_size = 10,\n",
    "                               seed = None, \n",
    "                               alpha = 0.0001, # dette er for relu ikke det samme som penaltien i scikit learn  \n",
    "                               activation = 'elu')\n",
    "\n",
    "    p = model.fit(X_train, y_train, X_test, y_test)\n",
    "    model.predict(X_test)\n",
    "    results.append(p)\n",
    "    print(\"Finished model: \" + str(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot results as a function of  epochs to see if a certain nr of nodes makes a difference. \n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "counter = 0\n",
    "for model in results:\n",
    "    mse_train = model.eval_['train_preform']\n",
    "    mse_test = model.eval_['valid_preform']\n",
    "    cost_train = model.eval_[\"cost_train\"]\n",
    "    r2_test = model.eval_['valid_preform_r2']\n",
    "\n",
    "    ax1 = plt.subplot(211)\n",
    "    plt.plot(r2_test, label = str(n_nodes[counter])) \n",
    "    plt.title(\"R2 score for different network architectures\", fontsize = 20)\n",
    "    plt.ylabel(\"R2 score\", fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Alle disse blir like.\n",
    "    ax2 = plt.subplot(212)\n",
    "    plt.plot(mse_test, label = str(n_nodes[counter])) \n",
    "    plt.title(\"MSE test for different one hidden layer architectures\", fontsize = 20)\n",
    "    plt.ylabel(\"MSE\", fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "plt.xlabel(\"Epochs\", fontsize = 15)\n",
    "plt.savefig(\"results/figures/NN_one_hidden_layer_elu_1000epochs_eta0.0001_featured_scaled_7.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = results[7].model_error\n",
    "plt.hist(error)\n",
    "plt.title(\"Histogram error using elu, 1000 epochs and one timestep of data\", fontsize = 15)\n",
    "plt.savefig(\"results/figures/NN_one_hidden_layer_elu_1000epochs_eta0.0001_featured_scaled_1_error.png\")\n",
    "# They all look the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running two hidden layers - combinations in [100, 125, 150, 175, 200]\n",
    "## Ran fro all permutations (includes ordering) didn't give a large manifestations ~0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1 = [100, 125]\n",
    "n_2 = [100, 150]\n",
    "n_7 = [100, 175]\n",
    "n_3 = [100, 200]\n",
    "\n",
    "n_4 = [125, 150]\n",
    "n_8 = [125, 175]\n",
    "n_5 = [125, 200]\n",
    "\n",
    "n_9 = [150, 175]\n",
    "n_6 = [150, 200]\n",
    "\n",
    "n_10 = [175,200]\n",
    "\n",
    "results = []\n",
    "\n",
    "#permutations_of_nodes = list(itertools.permutations(n_1)) + list(itertools.permutations(n_2)) \\\n",
    "#+ list(itertools.permutations(n_3))+ list(itertools.permutations(n_4)) + list(itertools.permutations(n_5))+ \\\n",
    "#list(itertools.permutations(n_6)) + list(itertools.permutations(n_7)) + list(itertools.permutations(n_8)) +list(itertools.permutations(n_9)) + list(itertools.permutations(n_10))\n",
    "\n",
    "combinations_of_nodes = comb = [n_1, n_2, n_3, n_4, n_5, n_6, n_7, n_8, n_9, n_10]\n",
    "\n",
    "for per in combinations_of_nodes:\n",
    "    model = NeuralNetRegressor(n_hidden = per,  \n",
    "                               epochs=1000, \n",
    "                               eta=0.0001,\n",
    "                               shuffle=True, \n",
    "                               batch_size=10,\n",
    "                               seed=None, \n",
    "                               alpha=0.0001, # Dette er for relu ikke det samme som penaltien i scikit learn  \n",
    "                               activation='sigmoid')\n",
    "\n",
    "    p = model.fit(X_train, y_train, X_test, y_test)\n",
    "    model.predict(X_test)\n",
    "    results.append(p)\n",
    "    print(\"finished model: \" + str(per) + \" performance \" + str(np.nanmean(model.eval_[\"valid_preform\"])) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "counter = 0\n",
    "for model in results:\n",
    "    mse_test = model.eval_['valid_preform']\n",
    "    r2_train = model.eval_['valid_preform_r2']\n",
    "\n",
    "    ax1 = plt.subplot(211)\n",
    "    plt.plot(r2_train[:200], label = str(combinations_of_nodes[counter])) \n",
    "    plt.title(\"R2 score for different neural network architectures\", fontsize = 20)\n",
    "    plt.ylabel(\"R2 score\", fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Alle disse blir like.\n",
    "    ax2 = plt.subplot(212)\n",
    "    plt.plot(mse_test[:200], label = str(combinations_of_nodes[counter])) \n",
    "    plt.title(\"MSE test for different neural network architectures\", fontsize = 20)\n",
    "    plt.ylabel(\"MSE\", fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "plt.xlabel(\"Epochs\", fontsize = 15)\n",
    "plt.savefig(\"results/figures/NN_two_hidden_layer_sigmoid_1000epochs_eta0.0001_StandardScaler_mse_r2_1_200epochs_7.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = results[0].model_error\n",
    "plt.hist(error)\n",
    "plt.title(\"Histogram error using sigmoid, two layers, 1000 epochs and one timestep of data\", fontsize = 15)\n",
    "plt.savefig(\"results/figures/NN_two_hidden_layer_sigmoid_1000epochs_eta0.0001_featured_scaled_1_error.png\")\n",
    "# They all look the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network three hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1 = [100, 125, 150]\n",
    "n_2 = [100, 125, 175]\n",
    "n_3 = [100, 125, 200]\n",
    "\n",
    "n_4 = [100, 150, 175]\n",
    "n_5 = [100, 150, 200]\n",
    "\n",
    "n_6 = [100, 175, 200]\n",
    "\n",
    "n_7 = [125, 150, 175]\n",
    "n_8 = [125, 175, 200]\n",
    "\n",
    "n_9 = [150, 175, 200]\n",
    "\n",
    "comb = [n_1, n_2, n_3, n_4, n_5, n_6, n_7, n_8, n_9]\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "#permutations_of_nodes = list(itertools.permutations(n_1)) + list(itertools.permutations(n_2)) \\\n",
    "#+ list(itertools.permutations(n_3))+ list(itertools.permutations(n_4)) + list(itertools.permutations(n_5))+ \\\n",
    "#list(itertools.permutations(n_6)) + list(itertools.permutations(n_7)) + list(itertools.permutations(n_8)) +list(itertools.permutations(n_9)) \n",
    "    \n",
    "for per in comb:\n",
    "    model = NeuralNetRegressor(n_hidden = per,  \n",
    "                               epochs=100, \n",
    "                               eta=0.0001,\n",
    "                               shuffle=True, \n",
    "                               batch_size=10,\n",
    "                               seed=None, \n",
    "                               alpha=0.0001, # Dette er for relu ikke det samme som penaltien i scikit learn  \n",
    "                               activation='sigmoid')\n",
    "\n",
    "    p = model.fit(X_train, y_train, X_test, y_test)\n",
    "    model.predict(X_test)\n",
    "    results.append(p)\n",
    "    print(\"finished model: \" + str(per) + \" performance \" + str(np.nanmean(model.eval_[\"valid_preform\"])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "counter = 0\n",
    "for model in results:\n",
    "    mse_test = model.eval_['valid_preform']\n",
    "    r2_train = model.eval_['valid_preform_r2']\n",
    "\n",
    "    ax1 = plt.subplot(211)\n",
    "    plt.plot(r2_train, label = str(comb[counter])) \n",
    "    plt.title(\"R2 score for different neural network architectures\", fontsize = 20)\n",
    "    plt.ylabel(\"R2 score\", fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Alle disse blir like.\n",
    "    ax2 = plt.subplot(212)\n",
    "    plt.plot(mse_test, label = str(comb[counter])) \n",
    "    plt.title(\"MSE test for different neural network architectures\", fontsize = 20)\n",
    "    plt.ylabel(\"MSE\", fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "plt.xlabel(\"Epochs\", fontsize = 15)\n",
    "plt.savefig(\"results/figures/NN_three_hidden_layer_sigmoid_1000epochs_eta0.0001_StandardScaler_mse_r2_7.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = results[0].model_error\n",
    "plt.hist(error)\n",
    "plt.title(\"Histogram error using sigmoid, three layers, 1000 epochs and one timestep of data\", fontsize = 15)\n",
    "plt.savefig(\"results/figures/NN_three_hidden_layer_sigmoid_1000epochs_eta0.0001_featured_scaled_1_error.png\")\n",
    "# They all look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net med fire hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1 = [100, 125, 150, 175]\n",
    "n_2 = [100, 125, 150, 200]\n",
    "n_3 = [125, 150, 175, 200]\n",
    "\n",
    "results = []\n",
    "\n",
    "#permutations_of_nodes = list(itertools.permutations(n_1)) + list(itertools.permutations(n_2)) + list(itertools.permutations(n_3))\n",
    "comb = [n_1, n_2, n_3]\n",
    "\n",
    "for per in comb:\n",
    "    model = NeuralNetRegressor(n_hidden = per,  \n",
    "                               epochs=100, \n",
    "                               eta=0.0001,\n",
    "                               shuffle=True, \n",
    "                               batch_size=10,\n",
    "                               seed=None, \n",
    "                               alpha=0.0001, # Dette er for relu ikke det samme som penaltien i scikit learn  \n",
    "                               activation='sigmoid')\n",
    "\n",
    "    p = model.fit(X_train, y_train, X_test, y_test)\n",
    "    model.predict(X_test)\n",
    "    results.append(p)\n",
    "    print(\"finished model: \" + str(per) + \" performance \" + str(np.nanmean(model.eval_[\"valid_preform\"])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "counter = 0\n",
    "for model in results:\n",
    "    mse_test = model.eval_['valid_preform']\n",
    "    r2_train = model.eval_['valid_preform_r2']\n",
    "\n",
    "    ax1 = plt.subplot(211)\n",
    "    plt.plot(r2_train, label = str(comb[counter])) \n",
    "    plt.title(\"R2 score for different neural network architectures\", fontsize = 20)\n",
    "    plt.ylabel(\"R2 score\", fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Alle disse blir like.\n",
    "    ax2 = plt.subplot(212)\n",
    "    plt.plot(mse_test, label = str(comb[counter])) \n",
    "    plt.title(\"MSE test for different neural network architectures\", fontsize = 20)\n",
    "    plt.ylabel(\"MSE\", fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "plt.xlabel(\"Epochs\", fontsize = 15)\n",
    "plt.savefig(\"results/figures/NN_four_hidden_layer_sigmoid_1000epochs_eta0.0001_StandardScaler_mse_r2_7.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = results[0].model_error\n",
    "plt.hist(error)\n",
    "plt.title(\"Histogram error using sigmoid, four layers, 1000 epochs and one timestep of data\", fontsize = 15)\n",
    "plt.savefig(\"results/figures/NN_four_hidden_layer_sigmoid_1000epochs_eta0.0001_featured_scaled_1_error.png\")\n",
    "# They all look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutations of the best four layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1 = [100, 125, 150, 175]\n",
    "n_2 = [100, 125, 150, 200]\n",
    "n_3 = [125, 150, 175, 200]\n",
    "\n",
    "results = []\n",
    "\n",
    "permutations_of_nodes = list(itertools.permutations(n_3)) #+ list(itertools.permutations(n_2)) + list(itertools.permutations(n_3))\n",
    "#comb = [n_1, n_2, n_3]\n",
    "\n",
    "for per in permutations_of_nodes:\n",
    "    model = NeuralNetRegressor(n_hidden = list(per),  \n",
    "                               epochs=100, \n",
    "                               eta=0.0001,\n",
    "                               shuffle=True, \n",
    "                               batch_size=10,\n",
    "                               seed=None, \n",
    "                               alpha=0.0001, # Dette er for relu ikke det samme som penaltien i scikit learn  \n",
    "                               activation='sigmoid')\n",
    "\n",
    "    p = model.fit(X_train, y_train, X_test, y_test)\n",
    "    model.predict(X_test)\n",
    "    results.append(p)\n",
    "    print(\"finished model: \" + str(per) + \" performance \" + str(np.nanmean(model.eval_[\"valid_preform\"])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "counter = 0\n",
    "for model in results:\n",
    "    mse_test = model.eval_['valid_preform']\n",
    "    r2_train = model.eval_['valid_preform_r2']\n",
    "\n",
    "    ax1 = plt.subplot(211)\n",
    "    plt.plot(r2_train, label = str(permutations_of_nodes[counter])) \n",
    "    plt.title(\"R2 score for different neural network architectures\", fontsize = 20)\n",
    "    plt.ylabel(\"R2 score\", fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Alle disse blir like.\n",
    "    ax2 = plt.subplot(212)\n",
    "    plt.plot(mse_test, label = str(permutations_of_nodes[counter])) \n",
    "    plt.title(\"MSE test for different neural network architectures\", fontsize = 20)\n",
    "    plt.ylabel(\"MSE\", fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "plt.xlabel(\"Epochs\", fontsize = 15)\n",
    "plt.savefig(\"results/figures/NN_four_hidden_layer_sigmoid_1000epochs_eta0.0001_StandardScaler_mse_r2_permutationd_7.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit MLP Regressor using one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#n_nodes = [10,30,50,100, 500]\n",
    "\"\"\"\n",
    "eta = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "lmd = [0.0001, 0.001, 0.01, 0.1, 1.0, 10]\n",
    "#epochs = [10,50,100] Scikit stopper på rett antall epochs\n",
    "batch_s = [1,10,50]\n",
    "\n",
    "# Scikit uses a different shape than we have implemented in our neural network.\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for e in epochs:\n",
    "\n",
    "for b in batch_s:\n",
    "    for et in eta:\n",
    "        for l in lmd:\n",
    "            mlp = MLPRegressor(hidden_layer_sizes=(30,), \n",
    "                               activation = 'relu', # identity’, ‘logistic’, ‘tanh’, ‘relu’\n",
    "                               solver = \"adam\", \n",
    "                               alpha = l, # penalty\n",
    "                               batch_size = b, \n",
    "                               learning_rate_init = et)\n",
    "\n",
    "            mlp.fit(X_train, y_train)\n",
    "            y_pred = mlp.predict(X_test)\n",
    "            #logistic activation uses the sigmoid function \n",
    "            \n",
    "            \n",
    "            # NEED TO TRANSFORM AND NORMALIZE THE RESULT. \n",
    "            #mse = NRMSE(y_pred, y_test)\n",
    "            #ajusted_r2 = A_R2(y_pred, y_test, n, p)\n",
    "            print(\" eta : \" + str(et) + \" lmd \"+ str(l) +\"    batch size : \" +  str(b)   + \"   mse is \" + str(mse) + \" a r2  \" + str(ajusted_r2))\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
