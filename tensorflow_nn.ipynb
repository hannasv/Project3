{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks using tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as n\n",
    "%matplotlib inline\n",
    "from utils import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflow_reg import nn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./files/\"\n",
    "filenames = [\"specific_humidity_Europa_sp.nc\", \"relative_humidity_Europa_sp.nc\", \"pressure_Europa_sp.nc\",  \n",
    "             \"temperature_Europa_sp.nc\", \"total_cloud_cover_Europa_sp.nc\"]\n",
    "\n",
    "\n",
    "cloud = n.Dataset(path + filenames[-1], \"r\")\n",
    "relative = n.Dataset(path + filenames[1], \"r\")\n",
    "specific = n.Dataset(path + filenames[0], \"r\")\n",
    "pressure = n.Dataset(path + filenames[2], \"r\")\n",
    "temperature = n.Dataset(path + filenames[3], \"r\")\n",
    "\n",
    "my_files = {'cloud':cloud, 'relative':relative, 'specific':specific, 'pressure':pressure, 'temperature':temperature}\n",
    "\n",
    "\n",
    "tcc = cloud.variables[\"tcc\"]\n",
    "rh = relative.variables[\"r\"]\n",
    "q = specific.variables[\"q\"]\n",
    "sp = pressure.variables[\"sp\"]\n",
    "t2m = temperature.variables[\"t2m\"]\n",
    "\n",
    "my_vars = {'tcc':tcc, 'rh':rh, 'q':q, 'sp':sp, 't2m':t2m}\n",
    "\n",
    "n_days = 1\n",
    "n_grid_boxes = tcc.shape[1]*tcc.shape[2]\n",
    "X = np.empty((n_grid_boxes*n_days*4, 4))\n",
    "y = np.empty((n_grid_boxes*tcc.shape[2]*n_days*4))\n",
    "\n",
    "\n",
    "\n",
    "for t in range(int(n_days)):\n",
    "    i = 0\n",
    "    for key, value in my_vars.items():\n",
    "        if key == 'tcc':\n",
    "            y[n_grid_boxes*t:n_grid_boxes*(t+1)] = np.array(value[t, : , :]).flatten()\n",
    "        elif key == 'rh' or key == 'q':\n",
    "            X[n_grid_boxes*t:n_grid_boxes*(t+1), i] = np.array(value[t, 0, : , :]).flatten()\n",
    "            i = i+1\n",
    "        else:\n",
    "            X[n_grid_boxes*t:n_grid_boxes*(t+1), i] = np.array(value[t, : , :]).flatten()\n",
    "            i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarize and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, split_size=0.3)\n",
    "\n",
    "Xn_train = (X_train - X_train.mean(axis=0))/X_train.std(axis=0)\n",
    "yn_train = (y_train - y_train.mean())/y_train.std()\n",
    "\n",
    "Xn_test = (X_test - X_train.mean(axis=0))/X_train.std(axis=0)\n",
    "yn_test = (y_test - y_train.mean())/y_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0430799946475558"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Xn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13152, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xn_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xn_train = tf.cast(Xn_train, tf.float32)  # convert to float 32 so that we can use tf.matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed forward neural net with 2 layers. Activation function: Sigmoid for all layers.\n",
    "\n",
    "TODO: write as function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "npred = Xn_train.shape[1]\n",
    "noutput = Xn_train.shape[0]\n",
    "n_nodes = [npred, 10, 50, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = tf.placeholder(\"float\")\n",
    "ys = tf.placeholder(\"float\")\n",
    "\n",
    "output = nn_model(xs, n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(output-ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_error = tf.reduce_sum(tf.square(tf.subtract(ys, tf.reduce_mean(ys))))\n",
    "\n",
    "unexplained_error = tf.reduce_sum(tf.square(tf.subtract(ys, output)))\n",
    "\n",
    "R_squared = tf.subtract(1.0, tf.divide(unexplained_error, total_error)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.001\n",
    "train = tf.train.GradientDescentOptimizer(eta).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Xn_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Cost train: 1.7214315 Cost test: 1.8372331\n",
      "Epoch : 1 Cost train: 1.7385457 Cost test: 1.848332\n",
      "Epoch : 2 Cost train: 1.7445581 Cost test: 1.8514938\n",
      "Epoch : 3 Cost train: 1.7505206 Cost test: 1.8398935\n",
      "Epoch : 4 Cost train: 1.7543066 Cost test: 1.8427784\n",
      "Epoch : 5 Cost train: 1.7569467 Cost test: 1.843484\n",
      "Epoch : 6 Cost train: 1.7588092 Cost test: 1.8450073\n",
      "Epoch : 7 Cost train: 1.761165 Cost test: 1.8458194\n",
      "Epoch : 8 Cost train: 1.7613595 Cost test: 1.8459971\n",
      "Epoch : 9 Cost train: 1.7612786 Cost test: 1.8460565\n",
      "Epoch : 10 Cost train: 1.7601812 Cost test: 1.8459514\n",
      "Epoch : 11 Cost train: 1.7599947 Cost test: 1.84606\n",
      "Epoch : 12 Cost train: 1.7597454 Cost test: 1.8458714\n",
      "Epoch : 13 Cost train: 1.7599583 Cost test: 1.8461429\n",
      "Epoch : 14 Cost train: 1.7597142 Cost test: 1.8464797\n",
      "Epoch : 15 Cost train: 1.7599053 Cost test: 1.8466016\n",
      "Epoch : 16 Cost train: 1.7600702 Cost test: 1.8462858\n",
      "Epoch : 17 Cost train: 1.7593733 Cost test: 1.8464155\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initiate session and initialize all vaiables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    #saver.restore(sess,'yahoo_dataset.ckpt')\n",
    "    c_t = []\n",
    "    c_test = []\n",
    "    r2_t = []\n",
    "    r2_test = []\n",
    "    for i in range(100):\n",
    "        for j in range(Xn_train.shape[0]):\n",
    "            sess.run([cost,train],feed_dict=    {xs:Xn_train[j,:].reshape(1,npred), ys:yn_train[j]})\n",
    "            # Run cost and train with each sample\n",
    "        c_t.append(sess.run(cost, feed_dict={xs:Xn_train,ys:yn_train}))\n",
    "        c_test.append(sess.run(cost, feed_dict={xs:Xn_test,ys:yn_test}))\n",
    "        print('Epoch :',i,'Cost train:',c_t[-1], 'Cost test:',c_test[-1])\n",
    "        print('Epoch :',i,'R2 train:',r2_t[-1], 'R2 test:',r2_test[-1])\n",
    "        r2_t.append(sess.run(R_squared, feed_dict={xs:Xn_train,ys:yn_train}))\n",
    "        r2_test.append(sess.run(R_squared, feed_dict={xs:Xn_test,ys:yn_test}))\n",
    "    pred = sess.run(output, feed_dict={xs:Xn_test})\n",
    "    # predict output of test data after training\n",
    "    print('Cost :',sess.run(cost, feed_dict={xs:Xn_test,ys:yn_test}))\n",
    "    #y_test = denormalize(df_test,y_test)\n",
    "    #pred = denormalize(df_test,pred)\n",
    "    #Denormalize data\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.plot(range(yn_test.shape[0]),yn_test,label=\"Original Data\")\n",
    "    plt.plot(range(yn_test.shape[0]),pred,label=\"Predicted Data\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel('Total Cloud Cover')\n",
    "    plt.xlabel('Points')\n",
    "    #plt.title('Total Cloud Cover')\n",
    "    plt.savefig('results/data_t0_2layers_10_50_split03.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(range(len(c_t)), c_t,label=\"MSE train\")\n",
    "    plt.plot(range(len(c_test)), c_test,label=\"MSE test\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epochs')\n",
    "    #plt.title('Cost vs. epochs')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(range(len(r2_t)), r2_t,label=\"R2 train\")\n",
    "    plt.plot(range(len(r2_test)), r2_test,label=\"R2 test\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylabel('R2')\n",
    "    plt.xlabel('Epochs')\n",
    "    #plt.title('R2 vs. epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/t0_2layers_10_50_split03.png')\n",
    "    plt.show()\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.48529008, -0.48529008, -0.48529008, -0.48529008, -0.48529008,\n",
       "       -0.48529008, -0.48529008, -0.48529008, -0.48529008])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yn_test[-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict={xs:Xn_train[j,:], ys:yn_train[j]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xn_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(Xn_train[j,:], [1, npred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nn_model(Xn_train, n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.get_shape().as_list()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tflow]",
   "language": "python",
   "name": "conda-env-tflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
